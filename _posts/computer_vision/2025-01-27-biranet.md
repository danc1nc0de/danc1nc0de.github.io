---
title: 快速了解一个网络：BIRA Net, Radar+RGB Attentive Fusion For Robust Object Detection In Autonomous Vehicles
date: 2025-01-27 22:00:00 +0800
categories: [Computer Vision]
tags: [快速了解一个网络, computer vision, 2d detection, radar-camera fusion]
---

> 以下内容偏向于记录个人学习过程及思考，请审慎阅读。
{: .prompt-info }

## 背景

融合radar and camera以获得更鲁棒的目标检测网络。

## 核心思想

将radar点云经过内外参反投到camera view上，通过resnet提features，接着通过maxpooling下采样后经过一个scSE Block（attention），再融合到camera image对应的FPN中。

## Pipeline

![biranet-radar-feature-extractor](assets/img/biranet-radar-feature-extractor.png)

![biranet-pipeline](assets/img/biranet-pipeline.png)

## 亿些细节

- radar点云反投camera时，点云的x, y对应feature map上的坐标，深度z则编码到对应的坐标位置。无radar点的位置编码为0
- radar features在哪个scale融合进camera，作者声明是经过一系列实验得出的。最终融合位置如上图pipeline所示
- radar-camera features融合方式为对应元素相加。
- scSE可以突出重要的局部空间信息及重要的channel，在这里作者应用在radar features提取上，以期望加强仅存在radar点云的区域。
- 基于radar点云产生anchor boxes。由于radar点可能是交点或者边点，这里作者考虑了几种不同的anchor生成方式，如下图。

![biranet-radar-anchor](assets/img/biranet-radar-anchor.png)

- 因为种种原因，不一定所有的目标都有对应的radar点，因此基于radar产生的anchor boxes很可能是不全的。这里作者提了一种融合anchor boxes的方法，简单来说就是同时基于camera image和radar产生anchor，如果radar对应的anchor box与gt的iou更大，则覆盖掉iou较小的基于image的anchor
- 作者使用nuScenes数据集，采用3d转2d方式获得2d框真值。由于很多遮挡严重的框也会被放出，这里作者通过nuScenes提供的visibility level进行了过滤，设置为2
- 本文的基础网络是Faster RCNN + FPN，预训练权重基于COCO数据集。

## 进一步了解

- [Faster RCNN](https://yinghao.info/posts/faster-rcnn/)
- [FPN](https://yinghao.info/posts/fpn/)
- Concurrent Spatial and Channel Squeeze & Excitation (scSE)

## 原文和代码

<https://arxiv.org/abs/2008.13642>

## 参考资料

无